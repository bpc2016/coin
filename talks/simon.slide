Pooled Bitcoin Mining with Go
A Distributed Solution

Busiso Chisala
Amateur Gopher 
busiso.chisala@gmail.com

* Agenda

The setup consists 

- a small cluster of *servers* (a single location?)

- many stawberry Pis as *clients* (across the globe)

- server, client software written in Go

* Bitcoin Mining

- users broadcast *transactions* to the bitcoin peer-to-peer network
- mining operation gathers a set of transactions to form potential  *next block*
- miner searches for a uint32 'nonce' determined by 
    - block contents 
    - going difficulty level
- if found, this solution is broadcast and the block tops the *blockchain*
- miner earns a reward in *coinbase bitcoin* (25BTC, this year goes down to 12.5)
- even if not found, _repeat_

*Note:* each loop is expected to last about ten minutes. 


* Pooled Mining

Mining is power intensive and requires a lot of computing effort 

- better to join a *pool*
- each member searches part of the nonce-space
- share the proceeds

* Distributed Pool

Our mining model will collect together miners across the internet

- client miners 
    mine,  get work from and send result to ..
- servers 
    responsible for groups of miners, get work from and  forward solution to ..
- conductor
    generates the block, runs a full node and talks to the bitcoin network

* Network Setup

.image network.svg


* Tools

Communication will involve bytes sequences and be bidirectional

- gRPC 
- Protobuf

* Proto file  /service/coin.proto

Specifies *rpc* communication between clients and servers, a set of interfaces

    package cpb;  // coin proto buffer

    // The mining service definition.
    service Coin {
        rpc Login (LoginRequest) returns (LoginReply) {}
        rpc GetWork (GetWorkRequest) returns (GetWorkReply) {}
        rpc Announce (AnnounceRequest) returns (AnnounceReply) {}
        rpc GetCancel (GetCancelRequest) returns (GetCancelReply) {}
        rpc IssueBlock (IssueBlockRequest) returns (IssueBlockReply) {}
        rpc GetResult (GetResultRequest) returns (GetResultReply) {}
    }

- `Login`, `GetWork`, `Announce`, `GetCancel` are implemented by _all_ actors
- IssueBlock, GetResult are implemented by _conductor_ and _servers_ only
- in effect, a conductor is an enhanced client

* .. creates /services/coin.pb.go

The following command  in the /service directory invokes the Go proto compiler:

    protoc *.proto --go_out=plugins=grpc:.

- produces the *package*cpb* Go source file `coin.pb.go` which has interfaces such as:

.code ../service/coin.pb.go /^type CoinClient interface/,/LoginReply, error\)/

- which we use in client/client.go

.code ../client/client.go /\/\/ login/,/\", err\)/


* Server side implementations

.code ../service/coin.pb.go /^type CoinServer interface/,/LoginReply, error\)/
is generally more substantial:

        type logger struct {
            sync.Mutex
            nextID   int
            loggedIn map[string]int
        }
        var users   logger
 
.code ../server/server.go /login OMIT/,/nigol OMIT/

* Server

* Server flow

.image server.svg

* Server flow - remarks

Note how this works

- the server line has the first three events duplicated, but the cycle is demarcated
- the bold red bars are the mining search operations
- this server has two clients, the second being *the* conductor


* Server  - main()

Looks so ...

    func main() {
        flag.Parse()
        users.loggedIn = make(map[string]int)
        users.nextID = -1
        *numMiners++ // to include the Conductor (EXTERNAL)

        port := fmt.Sprintf(":%d", 50051+*index)
        lis, err := net.Listen("tcp", port)
        fatalF("failed to listen", err)

                        :
                        

- reads flags, which includes *numMiners* if = 4 then in fact we handle 5 since we also service the *conductor*, which also feed data from other clients, and the external network 
- the *index* flag distinguishes this from other servers, sets the port for our RPC comms 
- *users* will track our clients

* Server  - main()

Then we set up our *server* object and initialise several channels
        
        signIn = make(chan string, *numMiners)  // register incoming miners
        signOut = make(chan string, *numMiners) // register miners receipt of cancel instructions
        blockchan = make(chan string, 1)        // transfer block data
        run.ch = make(chan struct{})            // signal to start mining
        resultchan = make(chan cpb.Win)         // transfer solution data

Note:  *run.ch* is a field of the run var which has type *lockable*

    type lockable struct {
        sync.Mutex
        winnerFound bool
        ch          chan struct{}
    }
we later fire up the server with

        s := new(server)
        g := grpc.NewServer()
        cpb.RegisterCoinServer(g, s)
        g.Serve(lis)

* Server - main() 
     

The main loop is inside the following anonymous goroutine:

.code ../server/server.go /loop OMIT/,/pool OMIT/

- *getNewBlock()* waits for a new block from the conductor
- *stop.Add(1)* refreshes a sync.Waitgroup in the *getCancel* goroutine
- *close(run.ch)* removes a block and synchronises the start of each run

* Server - GetWork()

Every client, on each work cycle, issues a request to this goroutine

.code ../server/server.go /gw OMIT/,/wg OMIT/

- registration allows *main()* to ensure that all clients have reached this point
- each *GetWork* blocks until *close(run.ch)* is called
- the server is responsible for customising the _coinbase_ of blocks it issues 
- all clients apart from the conductor start searches with their work

* Server - GetCancel()

At the same time that *main()* starts a run, it also refreshes the *stop* waitgroup used by GatCancel

.code ../server/server.go /cancel OMIT/,/lecnac OMIT/

- each client runs a getcancel request goroutine which allows notification of when to terminate searching
- the Conductor also 'searches' - on behalf of external miners
- the instruction comes from *Announce()* which checks *signOut* registration first
- the server index in the reply is used by the Conductor

* Server - Announce()

The Annouce goroutine is triggered by a client declaring success

.code ../server/server.go /\/\/ Announce/,/^}/

- a request coming after *run.winnerFound* is rejected with Ok=false
- the *stop.Done()* frees the getCancel replies to all clients

* Server - IssueBlock()

Conductor-specific communications 

.code ../server/server.go /func getNewBlock/,/^}/
.code ../server/server.go /\/\/ IssueBlock/,/^}/

- when the Conductor sends a new block to *IssueBlock*, the loop in *main()* accepts this via *GetNewBlock()* and restarts that cycle

* Server - GetResult()

The more critical Conductor-specific communication 

.code ../server/server.go /\/\/ GetResult/,/^}/

- uses the resultChan channels

- corresponding code in the Conductor:

.code ../conductor/conductor.go /func getResult/,/^}/


* Server - GetResult() on the Conductor

where *declareWin* does the following:

- handles messages from servers as well as 'internal' ones gnerated by 'wins' in the 'external' search

- returns the winning solution on channel *theWinner*

- uses a select on channel *lateEntry* to redirect subsequent claims from other servers

- confirms a win by sending a bogus win to the 'other' servers

- uses index = numServers to send this win in case of an external win, this time to *all* servers

* Server - GetResult() on the Conductor ...

.code ../conductor/conductor.go /func declareWin/,/^}/
